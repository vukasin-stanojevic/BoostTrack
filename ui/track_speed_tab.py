import streamlit as st
import cv2
import torch
import tempfile
import os
import numpy as np
import time
from external.adaptors.detector import Detector
from tracker.boost_track import BoostTrack
from PIL import Image
from streamlit_drawable_canvas import st_canvas
from ui.utils.roi import extract_thumbnail, is_inside_roi, calculate_roi_mid_length, roi_ui

ROI_POLYGON = np.array([(613, 19), (489, 37), (572, 329), (718, 296)], np.int32)
FPS = 30                   # ë¹„ë””ì˜¤ FPS (ê¸°ë³¸ê°’, ì‹¤ì œ FPSëŠ” ë¹„ë””ì˜¤ì—ì„œ ê°€ì ¸ì˜´)
UPDATE_INTERVAL = 3 * FPS  # 3ì´ˆë§ˆë‹¤ ì†ë„ ê°±ì‹  (í”„ë ˆì„ ë‹¨ìœ„)
MIN_DISTANCE = 5.0         # ìµœì†Œ ì´ë™ê±°ë¦¬ (5px ë¯¸ë§Œì€ 0ìœ¼ë¡œ ì²˜ë¦¬)

RESIZED_RATIO = 1 / 3
DRAWING_MODE = "rect"  
STROKE_WIDTH = 3
STROKE_COLOR = "#0000FF"
FILL_COLOR = "rgba(255, 255, 255, 0.0)"

def track_tab():
    roi_ui()

    if st.button("ëª¨ë¸ ì‹¤í–‰"):
        detector = Detector("yolox", "external/weights/bytetrack_x_mot20.tar", "custom")
        tracker = BoostTrack()

        cap = cv2.VideoCapture(st.session_video_path)
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        frame_rate = int(cap.get(cv2.CAP_PROP_FPS))
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        # ê²°ê³¼ ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì • -> ì„ì‹œ í´ë”ì— ì €ì¥ í›„, ë‚˜ì¤‘ì— data/outputìœ¼ë¡œ ì´ë™
        output_video_path = os.path.join(tempfile.gettempdir(), "output.mp4")
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))

        frame_display = st.empty()

        frame_count = 0
        last_update_frame = 0
        object_positions = {}   # {object_id: {"start": (x, y), "last": (x, y)}}
        object_speeds = {}      # {object_id: speed (px/s)}
        object_last_frame = {}  # {object_id: ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ëœ frame ë²ˆí˜¸}
        avg_speed_text = "Avg Speed: 0.00 px/s"
        speed_log = ""          # ì†ë„ ë¡œê·¸ë¥¼ ëˆ„ì í•´ì„œ ì €ì¥í•  ë¬¸ìì—´

        # ROI ì§ì„  ê¸¸ì´ ë° ì¤‘ê°„ ì„  ì¢Œí‘œ ê³„ì‚° -> ëŒ€ê¸°ì—´ ê¸¸ì´
        roi_length, left_mid, right_mid = calculate_roi_mid_length(st.session_state.roi_polygon)

        while cap.isOpened():
            start_time = time.time()
            ret, frame = cap.read()
            if not ret:
                break
            frame_count += 1

            # ROI ì˜ì—­ ê·¸ë¦¬ê¸° (ROI ë‹¤ê°í˜•ê³¼ ROI ì¤‘ê°„ ì„ (íŒŒë€ìƒ‰) í‘œì‹œ)
            cv2.polylines(frame, [st.session_state.roi_polygon], isClosed=True, color=(255, 0, 0), thickness=2)
            cv2.line(frame, left_mid, right_mid, (255, 0, 0), 2)

            # í”„ë ˆì„ í•´ìƒë„ ì¡°ì • ë° ìƒ‰ìƒ ë³€í™˜ (BGR â†’ RGB)
            img_numpy = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # YOLOX ì…ë ¥ìš© í…ì„œ ë³€í™˜ -> GPU ì‚¬ìš©
            img = torch.from_numpy(img_numpy).permute(2, 0, 1).float().cuda()
            img /= 255.0
            if len(img.shape) == 3:
                img = img.unsqueeze(0)

            # ê°ì²´ íƒì§€ë¥¼ ìœ„í•œ YOLOX ì¶”ë¡ 
            with torch.no_grad():
                outputs = detector.detect(img)
            if outputs is None:
                frame_display.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), channels="RGB", use_container_width=True)
                continue

            # BoostTrack++ ì¶”ì  ì‹¤í–‰
            targets = tracker.update(outputs, img, img_numpy, "custom_video")
            if isinstance(targets, np.ndarray):
                targets = [targets]

            # ROI ë‚´ë¶€ì— ìˆëŠ” ê°ì²´ë“¤ë§Œ ì²˜ë¦¬ (ë°”ìš´ë”©ë°•ìŠ¤, ID í‘œì‹œ ë° ê°ì²´ ìœ„ì¹˜ ì—…ë°ì´íŠ¸)
            for target in targets:
                if isinstance(target, np.ndarray) and target.ndim == 2:
                    for obj in target:
                        x1, y1, x2, y2, track_id = map(int, obj[:5])
                        center = ((x1 + x2) // 2, (y1 + y2) // 2)
                        if is_inside_roi(center, st.session_state.roi_polygon):
                            # ë°”ìš´ë”©ë°•ìŠ¤ ë° ID í‘œì‹œ (ì´ˆë¡ìƒ‰)
                            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                            cv2.putText(frame, f"ID: {track_id}", (x1, y1 - 10),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                            # ROI ë‚´ë¶€ì´ë©´ ê°ì²´ ìœ„ì¹˜ ì—…ë°ì´íŠ¸ ë° í˜„ì¬ í”„ë ˆì„ ê¸°ë¡
                            if track_id not in object_positions:
                                object_positions[track_id] = {"start": center, "last": center}
                            else:
                                object_positions[track_id]["last"] = center
                            object_last_frame[track_id] = frame_count
                            # ì´ë¯¸ ì†ë„ê°€ ê³„ì‚°ëœ ê²½ìš° ì†ë„ í…ìŠ¤íŠ¸ í‘œì‹œ
                            if track_id in object_speeds:
                                speed = object_speeds[track_id]
                                cv2.putText(frame, f"{speed:.2f} px/s", (x1, y2 + 20),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                        else:
                            # ROIë¥¼ ë²—ì–´ë‚œ ê°ì²´ëŠ” ì¦‰ì‹œ ì œê±°
                            if track_id in object_positions:
                                del object_positions[track_id]
                            if track_id in object_speeds:
                                del object_speeds[track_id]
                            if track_id in object_last_frame:
                                del object_last_frame[track_id]

            # 3ì´ˆë§ˆë‹¤ ì†ë„ ê°±ì‹  (UPDATE_INTERVALë§ˆë‹¤)
            if frame_count - last_update_frame >= UPDATE_INTERVAL:
                log_text = f"\nğŸ”¹ {frame_count // FPS}ì´ˆ ì‹œì  ì†ë„ ë¡œê·¸:\n"
                speeds = []
                # ROI ë‚´ë¶€ì— ì§€ì†ì ìœ¼ë¡œ ê°ì§€ëœ ê°   ì²´ë§Œ ê³ ë ¤: ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ í”„ë ˆì„ì´ í˜„ì¬ í”„ë ˆì„ì´ì–´ì•¼ í•¨.
                for track_id, pos in list(object_positions.items()):
                    if object_last_frame.get(track_id, 0) != frame_count:
                        del object_positions[track_id]
                        if track_id in object_speeds:
                            del object_speeds[track_id]
                        continue
                    start_pos = pos["start"]
                    last_pos = pos["last"]
                    displacement = np.sqrt((last_pos[0] - start_pos[0])**2 + (last_pos[1] - start_pos[1])**2)
                    speed = (displacement / 3.0) if displacement >= MIN_DISTANCE else 0
                    object_speeds[track_id] = speed
                    speeds.append(speed)
                    log_text += f"ê°ì²´ ID {track_id}: ì´ë™ ê±°ë¦¬ {displacement:.2f} px, ì†ë„ {speed:.2f} px/s\n"
                    # ë‹¤ìŒ êµ¬ê°„ ê³„ì‚°ì„ ìœ„í•´ ì‹œì‘ ìœ„ì¹˜ë¥¼ ìµœì‹  ìœ„ì¹˜ë¡œ ê°±ì‹ 
                    object_positions[track_id]["start"] = last_pos
                avg_speed = np.mean(speeds) if speeds else 0
                avg_speed_text = f"Avg Speed: {avg_speed:.2f} px/s"
                log_text += f"í‰ê·  ì†ë„: {avg_speed:.2f} px/s\n\n"
                speed_log += log_text
                last_update_frame = frame_count

            # í”„ë ˆì„ì— í‰ê·  ì†ë„ ë° FPS í‘œì‹œ
            cv2.putText(frame, avg_speed_text, (10, frame_height - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            elapsed_time = time.time() - start_time
            fps = 1.0 / elapsed_time if elapsed_time > 0 else 0
            cv2.putText(frame, f"FPS: {fps:.2f}", (10, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            # ê²°ê³¼ í”„ë ˆì„ ì €ì¥ ë° ì‹¤ì‹œê°„ í‘œì‹œ
            out.write(frame)
            frame_display.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), channels="RGB", use_container_width=True)

        cap.release()
        out.release()
        # os.remove(video_path)
        os.unlink(st.session_video_path)

        st.success("ëª¨ë¸ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

        # ìµœì¢… ê²°ê³¼ ê³„ì‚° (ROI ì§ì„  ê¸¸ì´ ë° ì˜ˆìƒ ëŒ€ê¸° ì‹œê°„)
        roi_length, _, _ = calculate_roi_mid_length(ROI_POLYGON)
        final_avg_speed = np.mean(list(object_speeds.values())) if object_speeds else 0
        if final_avg_speed > 0:
            estimated_wait_time = roi_length / final_avg_speed
        else:
            estimated_wait_time = float('inf')
        final_summary = f"ìµœì¢… í‰ê·  ì†ë„: {final_avg_speed:.2f} px/s\nROI ì§ì„  ê¸¸ì´: {roi_length:.2f} px\n"
        if estimated_wait_time == float('inf'):
            final_summary += "ì˜ˆìƒ ëŒ€ê¸° ì‹œê°„: ì¸¡ì • ë¶ˆê°€ (ì†ë„ 0)\n"
        else:
            final_summary += f"ì˜ˆìƒ ëŒ€ê¸° ì‹œê°„: {estimated_wait_time:.2f} ì´ˆ\n"

        # í† ê¸€(expander)ë¡œ ì†ë„ ë¡œê·¸ ë° ìµœì¢… ê²°ê³¼ í‘œì‹œ
        with st.expander("ì†ë„ ë¡œê·¸ ë° ìµœì¢… ê²°ê³¼ ë³´ê¸°"):
            st.text(speed_log + "\n===== ìµœì¢… ê²°ê³¼ =====\n" + final_summary)

        # ìµœì¢… ê²°ê³¼ ë¹„ë””ì˜¤ë¥¼ 'data/output' í´ë”ì— ì €ì¥
        final_result_folder = os.path.join("data", "output")
        os.makedirs(final_result_folder, exist_ok=True)
        final_output_path = os.path.join(final_result_folder, os.path.basename(output_video_path))
        os.rename(output_video_path, final_output_path)
        st.success(f"ë¹„ë””ì˜¤ê°€ {final_output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ìµœì¢… ê²°ê³¼ ë¡œê·¸ë¥¼ txt íŒŒì¼ë„ ì €ì¥
        log_file_path = os.path.join(final_result_folder, f"{os.path.splitext(os.path.basename(output_video_path))[0]}_speed_log.txt")
        with open(log_file_path, "w") as f:
            f.write(speed_log + "\n===== ìµœì¢… ê²°ê³¼ =====\n" + final_summary)
        st.success(f"ì†ë„ ë¡œê·¸ê°€ {log_file_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
